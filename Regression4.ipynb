{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dca1636-e3d4-42ad-b5bd-516ddcc1ed4b",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dc7d3-7acf-45e9-be04-be50d95a8eca",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection.\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + … + βpXp + ε\n",
    "where:\n",
    "\n",
    "Y: The response variable\n",
    "Xj: The jth predictor variable\n",
    "βj: The average effect on Y of a one unit increase in Xj, holding all other predictors fixed\n",
    "ε: The error term\n",
    "\n",
    "The values for β0, β1, B2, … , βp are chosen using the least square method, which minimizes the sum of squared residuals (RSS):\n",
    "\n",
    "RSS = Σ(yi – ŷi)2\n",
    "\n",
    "where:\n",
    "\n",
    "Σ: A greek symbol that means sum\n",
    "yi: The actual response value for the ith observation\n",
    "ŷi: The predicted response value based on the multiple linear regression model\n",
    "\n",
    "However, when the predictor variables are highly correlated then multicollinearity can become a problem. This can cause the coefficient estimates of the model to be unreliable and have high variance. That is, when the model is applied to a new set of data it hasn’t seen before, it’s likely to perform poorly.\n",
    "\n",
    "\n",
    "One way to get around this issue is to use a method known as lasso regression, which instead seeks to minimize the following:\n",
    "\n",
    "RSS + λΣ|βj|\n",
    "\n",
    "where j ranges from 1 to p and λ ≥ 0.\n",
    "\n",
    "This second term in the equation is known as a shrinkage penalty.\n",
    "\n",
    "When λ = 0, this penalty term has no effect and lasso regression produces the same coefficient estimates as least squares.\n",
    "\n",
    "However, as λ approaches infinity the shrinkage penalty becomes more influential and the predictor variables that aren’t importable in the model get shrunk towards zero and some even get dropped from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce2d0ee3-d454-44b2-84a9-53a48d6ae370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df=pd.read_csv('Algerian_forest_fires_dataset_UPDATE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f8f8ad8-de2c-449b-b532-85e965d6d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['day','month','year'],axis=1,inplace=True)\n",
    "## Encoding\n",
    "df['Classes']=np.where(df['Classes'].str.contains(\"not fire\"),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f74f9a36-27ba-4ad9-96e3-f43fd13d90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('FWI',axis=1)\n",
    "y=df['FWI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de4926cd-edb6-4c78-92b6-13a0ba14daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d215fa-d4c9-4460-813e-f0d3ec058236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: \n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa772418-6b22-479c-927d-16fbadf4968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182, 9), (61, 9))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_features=correlation(X_train,0.85)\n",
    "X_train.drop(corr_features,axis=1,inplace=True)\n",
    "X_test.drop(corr_features,axis=1,inplace=True)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f8cdba-f44a-4b04-9690-433511d64379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ab12332-1edb-4633-913c-8da4da858b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error 1.133175994914409\n",
      "R2 Score 0.9492020263112388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "lasso=Lasso()\n",
    "lasso.fit(X_train_scaled,y_train)\n",
    "y_pred=lasso.predict(X_test_scaled)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "score=r2_score(y_test,y_pred)\n",
    "print(\"Mean absolute error\", mae)\n",
    "print(\"R2 Score\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07d7ef-be9b-4b93-8076-bbceec5777b2",
   "metadata": {},
   "source": [
    "The advantage of lasso regression compared to least squares regression lies in the bias-variance tradeoff.\n",
    "\n",
    "Recall that mean squared error (MSE) is a metric we can use to measure the accuracy of a given model and it is calculated as:\n",
    "\n",
    "MSE = Var(f̂(x0)) + [Bias(f̂(x0))]2 + Var(ε)\n",
    "\n",
    "MSE = Variance + Bias2 + Irreducible error\n",
    "\n",
    "The basic idea of lasso regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall MSE.\n",
    "Notice that as λ increases, variance drops substantially with very little increase in bias. Beyond a certain point, though, variance decreases less rapidly and the shrinkage in the coefficients causes them to be significantly underestimated which results in a large increase in bias.\n",
    "\n",
    "We can see from the chart that the test MSE is lowest when we choose a value for λ that produces an optimal tradeoff between bias and variance.\n",
    "\n",
    "When λ = 0, the penalty term in lasso regression has no effect and thus it produces the same coefficient estimates as least squares. However, by increasing λ to a certain point we can reduce the overall test MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd44bcf-f646-4a37-a11a-254265e539b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88ac2ddf-512c-4412-9404-0ed47739628d",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa0f59-988c-4b09-aec2-ac804d4f457d",
   "metadata": {},
   "source": [
    "Trying to minimize the cost function, Lasso regression will automatically select those features that are useful, discarding the useless or redundant features. In Lasso regression, discarding a feature will make its coefficient equal to 0.\n",
    "\n",
    "So, the idea of using Lasso regression for feature selection purposes is very simple: we fit a Lasso regression on a scaled version of our dataset and we consider only those features that have a coefficient different from 0. Obviously, we first need to tune α hyperparameter in order to have the right kind of Lasso regression.\n",
    "\n",
    "That’s pretty easy and will make us easily detect the useful features and discard the useless features.\n",
    "\n",
    "The main advantage of using Lasso Regression for feature selection is its ability to automatically identify and select the most important features from a large pool of potential predictors. It does this by driving some coefficients to exactly zero, resulting in a sparse model. This helps simplify the model, improve interpretability, and prevent overfitting by excluding irrelevant or redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53170b-4beb-45c2-bfc0-abb32dadb44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e81c58-f279-41f1-b463-9627d1cfd806",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb64e5-0649-4f9c-ade1-6838fc302125",
   "metadata": {},
   "source": [
    "Y = β0 + β1X1 + β2X2 + … + βpXp + ε\n",
    "RSS = Σ(yi – ŷi)2\n",
    "\n",
    "Lasso regression works by shrinking the magnitude of the coefficients towards zero. It achieves this by adding a penalty term to the cost function that is proportional to the sum of the absolute values of the coefficients.\n",
    "\n",
    "The penalty term is defined as:\n",
    "RSS + λΣ|βj|\n",
    "\n",
    "where j ranges from 1 to p and λ ≥ 0.\n",
    "\n",
    "How to Interpret Lasso Regression Results\n",
    "Interpreting the results of a Lasso regression model can be challenging, but there are a few key steps that you can follow to make sense of the output.\n",
    "\n",
    "Step 1: Check the Model’s Coefficients\n",
    "The first step in interpreting the results of a Lasso regression model is to examine the values of the model’s coefficients. The coefficients represent the strength and direction of the relationship between the features and the target variable.\n",
    "\n",
    "In Lasso regression, some of the coefficients will be set to zero, which means that the corresponding feature has been excluded from the model. The non-zero coefficients represent the features that are most important for predicting the target variable.\n",
    "\n",
    "Step 2: Check the Model’s Performance Metrics\n",
    "The second step in interpreting the results of a Lasso regression model is to check the model’s performance metrics. These metrics provide an indication of how well the model is performing on the test data set.\n",
    "\n",
    "The most common performance metrics for regression models are:\n",
    "\n",
    "Mean Squared Error (MSE)\n",
    "R-squared (R^2)\n",
    "Mean Absolute Error (MAE)\n",
    "A good Lasso regression model should have a low MSE and MAE and a high R^2 value.\n",
    "\n",
    "Step 3: Check for Overfitting\n",
    "The third step in interpreting the results of a Lasso regression model is to check for overfitting. Overfitting occurs when the model is too complex and fits the training data too closely, resulting in poor performance on the test data set.\n",
    "\n",
    "One way to check for overfitting is to compare the model’s performance on the training and test data sets. If the model performs significantly better on the training data set than the test data set, it may be overfitting.\n",
    "\n",
    "Another way to check for overfitting is to use cross-validation. Cross-validation involves splitting the data set into multiple subsets and training the model on each subset while using the remaining subsets for testing. This can help to ensure that the model is not overfitting to any particular subset of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c0bbf1-3ef5-45b9-9384-f311c6e1b0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860929691203884"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, y_train: Training data and target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define alpha values to explore\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Initialize LassoCV with cross-validation folds\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5)\n",
    "\n",
    "# Fit LassoCV to training data\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = lasso_cv.alpha_\n",
    "\n",
    "# Retrain Lasso model with best alpha\n",
    "best_lasso_model = Lasso(alpha=best_alpha)\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_score = best_lasso_model.score(X_test, y_test)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09aff3-8aeb-4d79-87ee-3f99e4c292c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "545d06e3-306e-420f-a62f-3da6bb67256a",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922aaa23-d36a-4865-9f18-2aaae2695cec",
   "metadata": {},
   "source": [
    "In Lasso Regression, there is one main tuning parameter that you can adjust, which is the regularization strength parameter (often denoted as \"alpha/λ\"). The regularization strength controls the trade-off between fitting the data well and keeping the model simple. It affects how aggressively the Lasso algorithm penalizes the magnitude of the coefficients.\n",
    "\n",
    "--The alpha parameter controls the strength of the regularization. A higher value of alpha will result in more features being set to zero, which means they are excluded from the model.\n",
    "\n",
    "--When λ = 0, this penalty term has no effect and lasso regression produces the same coefficient estimates as least squares.\n",
    "\n",
    "--However, as λ approaches infinity the shrinkage penalty becomes more influential and the predictor variables that aren’t importable in the model get shrunk towards zero and some even get dropped from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2362e-05b6-4534-9e29-ff1ad19f4aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e4f37b-85b4-4acb-957f-0fe02a51734d",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb19e3-4acb-4e8e-a2ea-9b91cbc4b3c3",
   "metadata": {},
   "source": [
    "Lasso Regression, by itself, is designed for linear regression problems. However, it can be extended to handle non-linear regression problems by using non-linear transformations of the input features. This approach is often referred to as \"Lasso Regression with Polynomial Features\" or \"Lasso with Non-linear Basis Functions.\" Here's how you can adapt Lasso Regression for non-linear regression problems:\n",
    "Non-linear Transformations:\n",
    "Start by transforming the original features into non-linear forms. This can involve polynomial features, trigonometric functions, logarithmic functions, exponential functions, or any other suitable non-linear transformations.\n",
    "\n",
    "Feature Expansion:\n",
    "Create new features based on these non-linear transformations. For example, if you have a feature x, you can create new features like x^2, sin(x), log(x), etc. These new features capture the non-linear relationships between the original features and the target variable.\n",
    "\n",
    "Apply Lasso Regression:\n",
    "Use the expanded set of non-linear features as input to the Lasso Regression model. The regularization in Lasso Regression will still work with these non-linear features.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Tune the Lasso hyperparameter (alpha) to find the optimal level of regularization. Cross-validation can help you determine the best value of alpha that balances between fitting the data and preventing overfitting.\n",
    "\n",
    "Model Evaluation:\n",
    "Evaluate the performance of the trained Lasso model using appropriate evaluation metrics like Mean Squared Error (MSE), R-squared, or others depending on the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c67edcbb-7ad9-4054-8665-3bf63101cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9255923117832386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate non-linear data\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * np.sin(X) + np.random.randn(100, 1)\n",
    "\n",
    "# Create non-linear features\n",
    "X_poly = np.hstack([X, np.sin(X)])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Lasso model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1cd44-2873-43d9-bf5b-f2c77370d3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be3d704-90cd-4f26-9925-3e8fecc38733",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a461221-c9ea-47b3-8cc5-fb0844ef54b2",
   "metadata": {},
   "source": [
    "1.Regularization Term:\n",
    "\n",
    "\n",
    "Ridge Regression: Adds the squared magnitude of coefficients (L2 norm) as a penalty term to the loss function. The regularization term is given by α * Σ(coefficient^2), where α is the regularization parameter.\n",
    "\n",
    "Lasso Regression: Adds the absolute magnitude of coefficients (L1 norm) as a penalty term to the loss function. The regularization term is given by α * Σ|coefficient|, where α is the regularization parameter.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "2.Coefficient Shrinkage:\n",
    "\n",
    "Ridge Regression: Shrinks the coefficients towards zero, but they rarely become exactly zero. Ridge can significantly reduce the magnitude of coefficients, effectively preventing them from becoming too large.\n",
    "\n",
    "Lasso Regression: Can shrink coefficients all the way to zero, resulting in sparse models where some features are entirely excluded from the model. This leads to automatic feature selection.\n",
    "\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Ridge Regression: Does not perform automatic feature selection, as it retains all features in the model but with reduced magnitudes.\n",
    "\n",
    "Lasso Regression: Performs automatic feature selection by driving some coefficients to zero, favoring the most important features.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "3.Handling Multicollinearity:\n",
    "\n",
    "Ridge Regression: Handles multicollinearity (high correlation between predictors) well by spreading the impact of correlated features across multiple coefficients.\n",
    "\n",
    "Lasso Regression: Can struggle with multicollinearity, as it tends to pick one feature and discard the others in cases of high correlation.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------\n",
    "4.Tuning Parameter:\n",
    "\n",
    "Ridge Regression: The tuning parameter α controls the strength of regularization. Smaller α values result in weaker regularization.\n",
    "\n",
    "Lasso Regression: The tuning parameter α controls the strength of regularization. Larger α values result in weaker regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc427b-0047-4683-a439-a0217825ca6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e117c5-dcff-4f1a-8c05-fec46e0b540b",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780cd110-e4c4-4100-81da-6e74bbf9c5f8",
   "metadata": {},
   "source": [
    "Lasso Regression: Can struggle with multicollinearity, as it tends to pick one feature and discard the others in cases of high correlation.\n",
    "Lasso Regression can handle multicollinearity by selecting a subset of correlated features and driving the coefficients of others to zero. However, it might struggle in cases of extreme multicollinearity, and in those situations, other techniques or combinations of techniques might be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068151ab-4a09-4d12-b71c-e1de70487819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e19bfe54-161a-416c-a243-928ed87c5a17",
   "metadata": {},
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bf55c-48d8-4598-a5f1-9370c61442dd",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter, often denoted as λ (lambda), in Lasso Regression involves finding the value that strikes the right balance between model complexity and fitting the data. Cross-validation is a commonly used technique to determine the optimal λ value. Here's a step-by-step process:\n",
    "\n",
    "Divide Data into Training and Validation Sets:\n",
    "Split your dataset into a training set and a validation set. This is typically done using techniques like k-fold cross-validation or hold-out validation.\n",
    "\n",
    "Define a Range of λ Values:\n",
    "Choose a range of λ values to explore. You can use a logarithmic scale to cover a wide range of values, such as [0.001, 0.01, 0.1, 1, 10, 100].\n",
    "\n",
    "Cross-Validation Loop:\n",
    "For each λ value in your chosen range, perform the following steps:\n",
    "\n",
    "a. Train Lasso Regression:\n",
    "Train a Lasso Regression model using the training data and the current λ value.\n",
    "\n",
    "b. Validate Model:\n",
    "Evaluate the model's performance on the validation set using an appropriate metric (e.g., Mean Squared Error, R-squared).\n",
    "\n",
    "Select the Best λ:\n",
    "Choose the λ value that results in the best validation performance. This can be the λ value with the lowest validation error or the highest R-squared, depending on your problem.\n",
    "\n",
    "Refit Model with Optimal λ:\n",
    "After selecting the optimal λ value, retrain the Lasso Regression model using the entire training dataset and the chosen λ.\n",
    "\n",
    "Evaluate on Test Set:\n",
    "Finally, evaluate the performance of the model with the optimal λ on a separate test dataset that was not used during the cross-validation process. This gives you an unbiased estimate of the model's generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12ef9ba8-aa05-41f6-9c01-252f6c00df1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.026219437458035566"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, y_train: Training data and target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of lambda values to explore\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Initialize LassoCV with cross-validation folds\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5)\n",
    "\n",
    "# Fit LassoCV to training data\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best lambda value\n",
    "best_alpha = lasso_cv.alpha_\n",
    "\n",
    "# Retrain Lasso model with best lambda\n",
    "best_lasso_model = Lasso(alpha=best_alpha)\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_score = best_lasso_model.score(X_test, y_test)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0913ca-0137-48ee-8e4d-d0f24bbaa8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
